<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Affinities &mdash; TorchDR 0.0.0-alpha documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
    <link rel="shortcut icon" href="_static/logo.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=8e37ee27"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script>window.MathJax = {"tex": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Affinity" href="stubs/torchdr.Affinity.html" />
    <link rel="prev" title="BatchedAffinityMatcher" href="stubs/torchdr.affinity_matcher.BatchedAffinityMatcher.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >

          
          
          <a href="index.html">
            
              <img src="_static/torchdr_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="all.html">API and Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Affinities</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#base-structure">Base structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="stubs/torchdr.Affinity.html">Affinity</a></li>
<li class="toctree-l3"><a class="reference internal" href="stubs/torchdr.LogAffinity.html">LogAffinity</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#avoiding-memory-overflows-with-symbolic-lazy-tensors">Avoiding memory overflows with symbolic (lazy) tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="#affinities-based-on-entropic-projections">Affinities based on entropic projections</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-various-affinities">Other various affinities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.neighbor-embedding.html">Neighbor Embedding</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.notation.html">Math Notations</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.contributing.html">How to Contribute</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: white" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TorchDR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Affinities</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/torchdr.affinities.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-torchdr">
<span id="affinities"></span><span id="id1"></span><h1>Affinities<a class="headerlink" href="#module-torchdr" title="Link to this heading"></a></h1>
<p>Affinities are the essential building blocks of dimensionality reduction methods.
<code class="docutils literal notranslate"><span class="pre">TorchDR</span></code> provides a wide range of affinities, including basic ones such as <a class="reference internal" href="gen_modules/torchdr.GibbsAffinity.html#torchdr.GibbsAffinity" title="torchdr.GibbsAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">GibbsAffinity</span></code></a>, <a class="reference internal" href="gen_modules/torchdr.StudentAffinity.html#torchdr.StudentAffinity" title="torchdr.StudentAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">StudentAffinity</span></code></a> and <a class="reference internal" href="gen_modules/torchdr.ScalarProductAffinity.html#torchdr.ScalarProductAffinity" title="torchdr.ScalarProductAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">ScalarProductAffinity</span></code></a>.</p>
<section id="base-structure">
<h2>Base structure<a class="headerlink" href="#base-structure" title="Link to this heading"></a></h2>
<p>Affinities inherit the structure of the following <a class="reference internal" href="stubs/torchdr.Affinity.html#torchdr.Affinity" title="torchdr.Affinity"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Affinity()</span></code></a> class.</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="stubs/torchdr.Affinity.html#torchdr.Affinity" title="torchdr.Affinity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchdr.Affinity</span></code></a></p></td>
<td><p>Base class for affinity matrices.</p></td>
</tr>
</tbody>
</table>
<p>If computations can be performed in log domain, the <a class="reference internal" href="stubs/torchdr.LogAffinity.html#torchdr.LogAffinity" title="torchdr.LogAffinity"><code class="xref py py-meth docutils literal notranslate"><span class="pre">LogAffinity()</span></code></a> class should be used.</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="stubs/torchdr.LogAffinity.html#torchdr.LogAffinity" title="torchdr.LogAffinity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchdr.LogAffinity</span></code></a></p></td>
<td><p>Base class for affinity matrices in log space.</p></td>
</tr>
</tbody>
</table>
<p>All affinities have a <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit_transform()</span></code> method that can be used to compute the affinity matrix from a given data matrix. The affinity matrix is a <strong>square matrix of size (n, n)</strong> where n is the number of input samples.</p>
<p>Here is an example with the Gibbs affinity:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="kn">import</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">torchdr</span>
<span class="linenos">2</span>
<span class="linenos">3</span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="linenos">4</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="linenos">5</span><span class="n">affinity</span> <span class="o">=</span> <span class="n">torchdr</span><span class="o">.</span><span class="n">GibbsAffinity</span><span class="p">()</span>
<span class="linenos">6</span><span class="n">affinity_matrix</span> <span class="o">=</span> <span class="n">affinity</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
<p>They also have a <code class="xref py py-meth docutils literal notranslate"><span class="pre">get_batch()</span></code> method that can be called when the affinity is fitted. This method takes as input the indices of the samples that should be in the same batch. It returns the <strong>affinity matrix divided in blocks</strong> given by the batch indices. The output is of size <strong>(n_batch, batch_size, batch_size)</strong> where n_batch is the number of blocks and batch_size is the number of samples per block.</p>
<p>The number of blocks should be a divisor of the number of samples. Here is an example with 5 blocks of size 20 each:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 7</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="mi">5</span>
<span class="linenos"> 8</span><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="linenos"> 9</span><span class="n">batched_affinity_matrix</span> <span class="o">=</span> <span class="n">affinity</span><span class="o">.</span><span class="n">get_batch</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
<span class="linenos">10</span><span class="nb">print</span><span class="p">(</span><span class="n">batched_affinity_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>(5, 20, 20)
</pre></div>
</div>
</section>
<section id="avoiding-memory-overflows-with-symbolic-lazy-tensors">
<h2>Avoiding memory overflows with symbolic (lazy) tensors<a class="headerlink" href="#avoiding-memory-overflows-with-symbolic-lazy-tensors" title="Link to this heading"></a></h2>
<p>Affinities incur a quadratic memory cost, which can be particularly problematic when dealing with large numbers of samples, especially when using GPUs.</p>
<p>To prevent memory overflows, <code class="docutils literal notranslate"><span class="pre">TorchDR</span></code> relies on <code class="docutils literal notranslate"><span class="pre">KeOps</span></code> <a class="footnote-reference brackets" href="#id18" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>19<span class="fn-bracket">]</span></a> lazy tensors. These tensors are expressed as mathematical formulas, evaluated directly on the data samples. This symbolic representation allows computations to be performed without storing the entire matrix in memory, thereby effectively eliminating any memory limitation.</p>
<a class="reference internal image-reference" href="_images/symbolic_matrix.svg"><img alt="_images/symbolic_matrix.svg" class="align-center" src="_images/symbolic_matrix.svg" width="800" /></a>
<p>The above figure is taken from <a class="reference external" href="https://github.com/getkeops/keops/blob/main/doc/_static/symbolic_matrix.svg">here</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All <code class="docutils literal notranslate"><span class="pre">TorchDR</span></code> modules have a <code class="docutils literal notranslate"><span class="pre">keops</span></code> parameter that can be set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to use symbolic tensors. For small datasets, setting this parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code> allows the computation of the full affinity matrix directly in memory.</p>
</div>
</section>
<section id="affinities-based-on-entropic-projections">
<h2>Affinities based on entropic projections<a class="headerlink" href="#affinities-based-on-entropic-projections" title="Link to this heading"></a></h2>
<p>A widely used family of affinities focuses on <strong>controlling the entropy</strong> of the affinity matrix, which is a crucial aspect of SNE-related methods <a class="footnote-reference brackets" href="#id11" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<p>The first step is to ensure that each point has a unit mass, allowing the affinity matrix to be viewed as a <strong>Markov transition matrix</strong>. An <strong>adaptive bandwidth</strong> parameter then determines how the mass from each point spreads to its neighbors. The bandwidth is based on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> hyperparameter which controls the <strong>number of effective neighbors</strong> for each point.</p>
<p>The resulting affinities can be seen as a <strong>soft approximation of a k nearest neighbor graph</strong> where the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> plays the role of k. It allows capturing more subtleties than binary weights. Ultimately, the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> is an interpretable hyperparameter that determines which scale of dependencies is represented in the affinity.</p>
<p>The following table details the aspects controlled by various formulations of entropic affinities. <strong>Marginal</strong> refers to the row-wise control of mass. <strong>Entropy</strong> relates to the row-wise control of entropy dictated by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> hyperparameter.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Affinity (associated DR method)</strong></p></th>
<th class="head"><p><strong>Symmetry</strong></p></th>
<th class="head"><p><strong>Marginal</strong></p></th>
<th class="head"><p><strong>Entropy</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a> (<a class="reference internal" href="gen_modules/torchdr.SNE.html#torchdr.SNE" title="torchdr.SNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">SNE</span></code></a>) <a class="footnote-reference brackets" href="#id11" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.L2SymmetricEntropicAffinity.html#torchdr.L2SymmetricEntropicAffinity" title="torchdr.L2SymmetricEntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">L2SymmetricEntropicAffinity</span></code></a> (<a class="reference internal" href="gen_modules/torchdr.TSNE.html#torchdr.TSNE" title="torchdr.TSNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNE</span></code></a>) <a class="footnote-reference brackets" href="#id12" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.SinkhornAffinity.html#torchdr.SinkhornAffinity" title="torchdr.SinkhornAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SinkhornAffinity</span></code></a> (DOSNES) <a class="footnote-reference brackets" href="#id14" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> <a class="footnote-reference brackets" href="#id16" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.SymmetricEntropicAffinity.html#torchdr.SymmetricEntropicAffinity" title="torchdr.SymmetricEntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SymmetricEntropicAffinity</span></code></a> (SNEkhorn) <a class="footnote-reference brackets" href="#id13" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
<p>More details on these affinities can be found in the <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/8b54ecd9823fff6d37e61ece8f87e534-Paper-Conference.pdf">SNEkhorn paper</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The above table shows that <a class="reference internal" href="gen_modules/torchdr.SymmetricEntropicAffinity.html#torchdr.SymmetricEntropicAffinity" title="torchdr.SymmetricEntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SymmetricEntropicAffinity</span></code></a> is the proper symmetric version of <a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a>.
However <a class="reference internal" href="gen_modules/torchdr.L2SymmetricEntropicAffinity.html#torchdr.L2SymmetricEntropicAffinity" title="torchdr.L2SymmetricEntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">L2SymmetricEntropicAffinity</span></code></a> is more efficient to compute and does not require choosing a learning rate. Hence it can be a useful approximation in practice.</p>
</div>
</section>
<section id="other-various-affinities">
<h2>Other various affinities<a class="headerlink" href="#other-various-affinities" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">TorchDR</span></code> features other affinities that can be used in various contexts.</p>
<p>For instance, the UMAP <a class="footnote-reference brackets" href="#id15" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> algorithm relies on the affinities <a class="reference internal" href="gen_modules/torchdr.UMAPAffinityIn.html#torchdr.UMAPAffinityIn" title="torchdr.UMAPAffinityIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityIn</span></code></a> for the input data and <a class="reference internal" href="gen_modules/torchdr.UMAPAffinityOut.html#torchdr.UMAPAffinityOut" title="torchdr.UMAPAffinityOut"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityOut</span></code></a> in the embedding space. <a class="reference internal" href="gen_modules/torchdr.UMAPAffinityIn.html#torchdr.UMAPAffinityIn" title="torchdr.UMAPAffinityIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityIn</span></code></a> follows a similar construction as entropic affinities to ensure a constant number of effective neighbors, with <code class="xref py py-attr docutils literal notranslate"><span class="pre">n_neighbors</span></code> playing the role of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> hyperparameter.</p>
<p>Another example is the doubly stochastic normalization of a base affinity under the <span class="math notranslate nohighlight">\(\ell_2\)</span> geometry that has recently been proposed for DR <a class="footnote-reference brackets" href="#id17" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>. This method is analogous to <a class="reference internal" href="gen_modules/torchdr.SinkhornAffinity.html#torchdr.SinkhornAffinity" title="torchdr.SinkhornAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SinkhornAffinity</span></code></a> where the Shannon entropy is replaced by the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm to recover a sparse affinity.
It is available at <a class="reference internal" href="gen_modules/torchdr.DoublyStochasticQuadraticAffinity.html#torchdr.DoublyStochasticQuadraticAffinity" title="torchdr.DoublyStochasticQuadraticAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">DoublyStochasticQuadraticAffinity</span></code></a>.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading"></a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id11" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>)</span>
<p>Geoffrey Hinton, Sam Roweis (2002). <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf">Stochastic Neighbor Embedding</a>. Advances in Neural Information Processing Systems 15 (NeurIPS).</p>
</aside>
<aside class="footnote brackets" id="id12" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p>Laurens van der Maaten, Geoffrey Hinton (2008). <a class="reference external" href="https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf?fbcl">Visualizing Data using t-SNE</a>. The Journal of Machine Learning Research 9.11 (JMLR).</p>
</aside>
<aside class="footnote brackets" id="id13" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">3</a><span class="fn-bracket">]</span></span>
<p>Hugues Van Assel, Titouan Vayer, Rémi Flamary, Nicolas Courty (2023). <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/8b54ecd9823fff6d37e61ece8f87e534-Paper-Conference.pdf">SNEkhorn: Dimension Reduction with Symmetric Entropic Affinities</a>. Advances in Neural Information Processing Systems 36 (NeurIPS).</p>
</aside>
<aside class="footnote brackets" id="id14" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">5</a><span class="fn-bracket">]</span></span>
<p>Richard Sinkhorn, Paul Knopp (1967). <a class="reference external" href="https://msp.org/pjm/1967/21-2/pjm-v21-n2-p14-p.pdf">Concerning nonnegative matrices and doubly stochastic matrices</a>. Pacific Journal of Mathematics, 21(2), 343-348.</p>
</aside>
<aside class="footnote brackets" id="id15" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">8</a><span class="fn-bracket">]</span></span>
<p>Leland McInnes, John Healy, James Melville (2018). <a class="reference external" href="https://arxiv.org/abs/1802.03426">UMAP: Uniform manifold approximation and projection for dimension reduction</a>. arXiv preprint arXiv:1802.03426.</p>
</aside>
<aside class="footnote brackets" id="id16" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">9</a><span class="fn-bracket">]</span></span>
<p>Yao Lu, Jukka Corander, Zhirong Yang (2019). <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0167865518305099">Doubly Stochastic Neighbor Embedding on Spheres</a>. Pattern Recognition Letters 128 : 100-106.</p>
</aside>
<aside class="footnote brackets" id="id17" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">10</a><span class="fn-bracket">]</span></span>
<p>Stephen Zhang, Gilles Mordant, Tetsuya Matsumoto, Geoffrey Schiebinger (2023). <a class="reference external" href="https://arxiv.org/abs/2307.09816">Manifold Learning with Sparse Regularised Optimal Transport</a>. arXiv preprint.</p>
</aside>
<aside class="footnote brackets" id="id18" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">19</a><span class="fn-bracket">]</span></span>
<p>Charlier, B., Feydy, J., Glaunes, J. A., Collin, F. D., &amp; Durif, G. (2021). <a class="reference external" href="https://www.jmlr.org/papers/volume22/20-275/20-275.pdf">Kernel Operations on the GPU, with Autodiff, without Memory Overflows</a>. Journal of Machine Learning Research (JMLR).</p>
</aside>
</aside>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="stubs/torchdr.affinity_matcher.BatchedAffinityMatcher.html" class="btn btn-neutral float-left" title="BatchedAffinityMatcher" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="stubs/torchdr.Affinity.html" class="btn btn-neutral float-right" title="Affinity" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, TorchDR team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>