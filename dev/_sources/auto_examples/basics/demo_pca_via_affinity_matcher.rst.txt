
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/basics/demo_pca_via_affinity_matcher.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_basics_demo_pca_via_affinity_matcher.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_basics_demo_pca_via_affinity_matcher.py:


PCA via SVD and via AffinityMatcher
===================================

We show how to compute a PCA embedding using the closed form
and using the AffinityMatcher class. Both approaches lead to the same solution.

.. GENERATED FROM PYTHON SOURCE LINES 9-16

.. code-block:: Python


    import matplotlib.pyplot as plt
    from sklearn.datasets import load_digits

    from torchdr.spectral import PCA
    from torchdr import AffinityMatcher, ScalarProductAffinity








.. GENERATED FROM PYTHON SOURCE LINES 17-21

Load toy images
---------------

First, let's load 5 classes of the digits dataset from sklearn.

.. GENERATED FROM PYTHON SOURCE LINES 21-26

.. code-block:: Python


    digits = load_digits(n_class=5)
    X = digits.data
    X = X - X.mean(0)








.. GENERATED FROM PYTHON SOURCE LINES 27-33

PCA via SVD
-----------

Let us perform PCA using the closed form solution given by the
Singular Value Decomposition (SVD).
In ``Torchdr``, it is available at :class:`torchdr.PCA`.

.. GENERATED FROM PYTHON SOURCE LINES 33-42

.. code-block:: Python


    Z_svd = PCA(n_components=2).fit_transform(X)

    plt.figure()
    plt.scatter(Z_svd[:, 0], Z_svd[:, 1], c=digits.target)
    plt.title("PCA via SVD")
    plt.show()





.. image-sg:: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_001.png
   :alt: PCA via SVD
   :srcset: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 43-52

PCA via AffinityMatcher
-----------------------

Now, let us perform PCA using the AffinityMatcher class
:class:`torchdr.AffinityMatcher`
as well as the scalar product affinity
:class:`torchdr.ScalarProductAffinity`
for both input data and embeddings,
and the square loss as global objective.

.. GENERATED FROM PYTHON SOURCE LINES 52-71

.. code-block:: Python


    model = AffinityMatcher(
        n_components=2,
        affinity_in=ScalarProductAffinity(),
        affinity_out=ScalarProductAffinity(),
        loss_fn="square_loss",
        init="normal",
        lr=1e1,
        max_iter=50,
        keops=False,
    )
    Z_am = model.fit_transform(X)

    plt.figure()
    plt.scatter(Z_am[:, 0], Z_am[:, 1], c=digits.target)
    plt.title("PCA via AffinityMatcher")
    plt.show()





.. image-sg:: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_002.png
   :alt: PCA via AffinityMatcher
   :srcset: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/50 [00:00<?, ?it/s]    Loss : 1.35e+11 | Grad norm : 3.06e+02 :   0%|          | 0/50 [00:00<?, ?it/s]    Loss : 9.42e+10 | Grad norm : 1.57e+08 :   0%|          | 0/50 [00:00<?, ?it/s]    Loss : 9.86e+10 | Grad norm : 3.46e+08 :   0%|          | 0/50 [00:00<?, ?it/s]    Loss : 9.86e+10 | Grad norm : 3.46e+08 :   6%|▌         | 3/50 [00:00<00:03, 13.43it/s]    Loss : 7.49e+10 | Grad norm : 1.21e+08 :   6%|▌         | 3/50 [00:00<00:03, 13.43it/s]    Loss : 7.28e+10 | Grad norm : 1.69e+08 :   6%|▌         | 3/50 [00:00<00:03, 13.43it/s]    Loss : 7.28e+10 | Grad norm : 1.69e+08 :  10%|█         | 5/50 [00:00<00:05,  8.96it/s]    Loss : 6.51e+10 | Grad norm : 1.87e+08 :  10%|█         | 5/50 [00:00<00:05,  8.96it/s]    Loss : 5.72e+10 | Grad norm : 1.99e+08 :  10%|█         | 5/50 [00:00<00:05,  8.96it/s]    Loss : 5.72e+10 | Grad norm : 1.99e+08 :  14%|█▍        | 7/50 [00:00<00:05,  7.87it/s]    Loss : 5.15e+10 | Grad norm : 2.21e+08 :  14%|█▍        | 7/50 [00:00<00:05,  7.87it/s]    Loss : 5.15e+10 | Grad norm : 2.21e+08 :  16%|█▌        | 8/50 [00:00<00:05,  8.24it/s]    Loss : 4.53e+10 | Grad norm : 1.86e+08 :  16%|█▌        | 8/50 [00:01<00:05,  8.24it/s]    Loss : 4.53e+10 | Grad norm : 1.86e+08 :  18%|█▊        | 9/50 [00:01<00:04,  8.61it/s]    Loss : 4.40e+10 | Grad norm : 1.61e+08 :  18%|█▊        | 9/50 [00:01<00:04,  8.61it/s]    Loss : 4.58e+10 | Grad norm : 1.62e+08 :  18%|█▊        | 9/50 [00:01<00:04,  8.61it/s]    Loss : 4.58e+10 | Grad norm : 1.62e+08 :  22%|██▏       | 11/50 [00:01<00:04,  9.14it/s]    Loss : 4.63e+10 | Grad norm : 1.51e+08 :  22%|██▏       | 11/50 [00:01<00:04,  9.14it/s]    Loss : 4.49e+10 | Grad norm : 1.35e+08 :  22%|██▏       | 11/50 [00:01<00:04,  9.14it/s]    Loss : 4.49e+10 | Grad norm : 1.35e+08 :  26%|██▌       | 13/50 [00:01<00:03,  9.46it/s]    Loss : 4.26e+10 | Grad norm : 1.19e+08 :  26%|██▌       | 13/50 [00:01<00:03,  9.46it/s]    Loss : 4.26e+10 | Grad norm : 1.19e+08 :  28%|██▊       | 14/50 [00:01<00:03,  9.57it/s]    Loss : 4.10e+10 | Grad norm : 1.10e+08 :  28%|██▊       | 14/50 [00:01<00:03,  9.57it/s]    Loss : 4.10e+10 | Grad norm : 1.10e+08 :  30%|███       | 15/50 [00:01<00:03,  9.66it/s]    Loss : 4.04e+10 | Grad norm : 1.13e+08 :  30%|███       | 15/50 [00:01<00:03,  9.66it/s]    Loss : 4.04e+10 | Grad norm : 1.13e+08 :  32%|███▏      | 16/50 [00:01<00:03,  9.74it/s]    Loss : 4.00e+10 | Grad norm : 1.14e+08 :  32%|███▏      | 16/50 [00:01<00:03,  9.74it/s]    Loss : 4.00e+10 | Grad norm : 1.14e+08 :  34%|███▍      | 17/50 [00:01<00:03,  9.80it/s]    Loss : 3.93e+10 | Grad norm : 1.07e+08 :  34%|███▍      | 17/50 [00:02<00:03,  9.80it/s]    Loss : 3.93e+10 | Grad norm : 1.07e+08 :  36%|███▌      | 18/50 [00:02<00:04,  7.78it/s]    Loss : 3.87e+10 | Grad norm : 9.93e+07 :  36%|███▌      | 18/50 [00:02<00:04,  7.78it/s]    Loss : 3.87e+10 | Grad norm : 9.93e+07 :  38%|███▊      | 19/50 [00:02<00:03,  8.29it/s]    Loss : 3.82e+10 | Grad norm : 9.29e+07 :  38%|███▊      | 19/50 [00:02<00:03,  8.29it/s]    Loss : 3.82e+10 | Grad norm : 9.29e+07 :  40%|████      | 20/50 [00:02<00:03,  8.71it/s]    Loss : 3.75e+10 | Grad norm : 8.65e+07 :  40%|████      | 20/50 [00:02<00:03,  8.71it/s]    Loss : 3.66e+10 | Grad norm : 7.81e+07 :  40%|████      | 20/50 [00:02<00:03,  8.71it/s]    Loss : 3.66e+10 | Grad norm : 7.81e+07 :  44%|████▍     | 22/50 [00:02<00:03,  9.25it/s]    Loss : 3.59e+10 | Grad norm : 7.18e+07 :  44%|████▍     | 22/50 [00:02<00:03,  9.25it/s]    Loss : 3.59e+10 | Grad norm : 7.18e+07 :  46%|████▌     | 23/50 [00:02<00:02,  9.38it/s]    Loss : 3.58e+10 | Grad norm : 7.24e+07 :  46%|████▌     | 23/50 [00:02<00:02,  9.38it/s]    Loss : 3.58e+10 | Grad norm : 7.24e+07 :  48%|████▊     | 24/50 [00:02<00:03,  7.68it/s]    Loss : 3.59e+10 | Grad norm : 7.42e+07 :  48%|████▊     | 24/50 [00:02<00:03,  7.68it/s]    Loss : 3.59e+10 | Grad norm : 7.42e+07 :  50%|█████     | 25/50 [00:02<00:03,  8.17it/s]    Loss : 3.58e+10 | Grad norm : 7.10e+07 :  50%|█████     | 25/50 [00:02<00:03,  8.17it/s]    Loss : 3.58e+10 | Grad norm : 7.10e+07 :  52%|█████▏    | 26/50 [00:02<00:02,  8.60it/s]    Loss : 3.56e+10 | Grad norm : 6.54e+07 :  52%|█████▏    | 26/50 [00:03<00:02,  8.60it/s]    Loss : 3.54e+10 | Grad norm : 6.10e+07 :  52%|█████▏    | 26/50 [00:03<00:02,  8.60it/s]    Loss : 3.54e+10 | Grad norm : 6.10e+07 :  56%|█████▌    | 28/50 [00:03<00:01, 11.47it/s]    Loss : 3.51e+10 | Grad norm : 5.77e+07 :  56%|█████▌    | 28/50 [00:03<00:01, 11.47it/s]    Loss : 3.48e+10 | Grad norm : 5.48e+07 :  56%|█████▌    | 28/50 [00:03<00:01, 11.47it/s]    Loss : 3.48e+10 | Grad norm : 5.48e+07 :  60%|██████    | 30/50 [00:03<00:01, 10.87it/s]    Loss : 3.44e+10 | Grad norm : 5.14e+07 :  60%|██████    | 30/50 [00:03<00:01, 10.87it/s]    Loss : 3.42e+10 | Grad norm : 4.75e+07 :  60%|██████    | 30/50 [00:03<00:01, 10.87it/s]    Loss : 3.42e+10 | Grad norm : 4.75e+07 :  64%|██████▍   | 32/50 [00:03<00:02,  8.84it/s]    Loss : 3.40e+10 | Grad norm : 4.57e+07 :  64%|██████▍   | 32/50 [00:03<00:02,  8.84it/s]    Loss : 3.40e+10 | Grad norm : 4.49e+07 :  64%|██████▍   | 32/50 [00:03<00:02,  8.84it/s]    Loss : 3.40e+10 | Grad norm : 4.49e+07 :  68%|██████▊   | 34/50 [00:03<00:01,  9.24it/s]    Loss : 3.40e+10 | Grad norm : 4.24e+07 :  68%|██████▊   | 34/50 [00:03<00:01,  9.24it/s]    Loss : 3.40e+10 | Grad norm : 4.06e+07 :  68%|██████▊   | 34/50 [00:04<00:01,  9.24it/s]    Loss : 3.40e+10 | Grad norm : 4.06e+07 :  72%|███████▏  | 36/50 [00:04<00:01,  8.23it/s]    Loss : 3.40e+10 | Grad norm : 4.06e+07 :  72%|███████▏  | 36/50 [00:04<00:01,  8.23it/s]    Loss : 3.40e+10 | Grad norm : 4.06e+07 :  74%|███████▍  | 37/50 [00:04<00:01,  8.51it/s]    Loss : 3.39e+10 | Grad norm : 3.87e+07 :  74%|███████▍  | 37/50 [00:04<00:01,  8.51it/s]    Loss : 3.39e+10 | Grad norm : 3.87e+07 :  76%|███████▌  | 38/50 [00:04<00:01,  8.79it/s]    Loss : 3.37e+10 | Grad norm : 3.43e+07 :  76%|███████▌  | 38/50 [00:04<00:01,  8.79it/s]    Loss : 3.37e+10 | Grad norm : 3.43e+07 :  78%|███████▊  | 39/50 [00:04<00:01,  9.04it/s]    Loss : 3.35e+10 | Grad norm : 3.10e+07 :  78%|███████▊  | 39/50 [00:04<00:01,  9.04it/s]    Loss : 3.35e+10 | Grad norm : 3.10e+07 :  80%|████████  | 40/50 [00:04<00:01,  9.25it/s]    Loss : 3.35e+10 | Grad norm : 3.06e+07 :  80%|████████  | 40/50 [00:04<00:01,  9.25it/s]    Loss : 3.35e+10 | Grad norm : 3.06e+07 :  82%|████████▏ | 41/50 [00:04<00:00,  9.41it/s]    Loss : 3.34e+10 | Grad norm : 3.05e+07 :  82%|████████▏ | 41/50 [00:04<00:00,  9.41it/s]    Loss : 3.34e+10 | Grad norm : 3.05e+07 :  84%|████████▍ | 42/50 [00:04<00:00,  9.56it/s]    Loss : 3.34e+10 | Grad norm : 2.91e+07 :  84%|████████▍ | 42/50 [00:04<00:00,  9.56it/s]    Loss : 3.34e+10 | Grad norm : 2.91e+07 :  86%|████████▌ | 43/50 [00:04<00:00,  9.67it/s]    Loss : 3.33e+10 | Grad norm : 2.70e+07 :  86%|████████▌ | 43/50 [00:04<00:00,  9.67it/s]    Loss : 3.33e+10 | Grad norm : 2.56e+07 :  86%|████████▌ | 43/50 [00:04<00:00,  9.67it/s]    Loss : 3.33e+10 | Grad norm : 2.56e+07 :  90%|█████████ | 45/50 [00:04<00:00,  9.83it/s]    Loss : 3.33e+10 | Grad norm : 2.49e+07 :  90%|█████████ | 45/50 [00:05<00:00,  9.83it/s]    Loss : 3.33e+10 | Grad norm : 2.49e+07 :  92%|█████████▏| 46/50 [00:05<00:00,  6.71it/s]    Loss : 3.33e+10 | Grad norm : 2.31e+07 :  92%|█████████▏| 46/50 [00:05<00:00,  6.71it/s]    Loss : 3.33e+10 | Grad norm : 2.31e+07 :  94%|█████████▍| 47/50 [00:05<00:00,  6.17it/s]    Loss : 3.32e+10 | Grad norm : 2.16e+07 :  94%|█████████▍| 47/50 [00:05<00:00,  6.17it/s]    Loss : 3.32e+10 | Grad norm : 2.16e+07 :  96%|█████████▌| 48/50 [00:05<00:00,  6.87it/s]    Loss : 3.32e+10 | Grad norm : 2.17e+07 :  96%|█████████▌| 48/50 [00:05<00:00,  6.87it/s]    Loss : 3.32e+10 | Grad norm : 2.17e+07 :  98%|█████████▊| 49/50 [00:05<00:00,  7.46it/s]    Loss : 3.32e+10 | Grad norm : 2.12e+07 :  98%|█████████▊| 49/50 [00:05<00:00,  7.46it/s]    Loss : 3.32e+10 | Grad norm : 2.12e+07 : 100%|██████████| 50/50 [00:05<00:00,  8.73it/s]




.. GENERATED FROM PYTHON SOURCE LINES 72-74

We can see that we obtain the same PCA embedding (up to a rotation) using
both methods.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 7.185 seconds)


.. _sphx_glr_download_auto_examples_basics_demo_pca_via_affinity_matcher.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_pca_via_affinity_matcher.ipynb <demo_pca_via_affinity_matcher.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_pca_via_affinity_matcher.py <demo_pca_via_affinity_matcher.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
