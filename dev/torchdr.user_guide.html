

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>User Guide &mdash; TorchDR 0.0.0-alpha documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
    <link rel="shortcut icon" href="_static/logo.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=8e37ee27"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script>window.MathJax = {"tex": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Affinity" href="stubs/torchdr.Affinity.html" />
    <link rel="prev" title="Quick Start Guide" href="torchdr.quick_start.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: white" >

          
          
          <a href="index.html">
            
              <img src="_static/torchdr_logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Torch Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.quick_start.html">Quick Start Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">User Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#general-formulation-of-dimensionality-reduction">General Formulation of Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#torch-gpu-support-and-automatic-differentiation">Torch GPU support and automatic differentiation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#avoiding-memory-overflows-with-keops-symbolic-lazy-tensors">Avoiding memory overflows with KeOps symbolic (lazy) tensors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#affinities">Affinities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#base-structure">Base structure</a><ul>
<li class="toctree-l4"><a class="reference internal" href="stubs/torchdr.Affinity.html">Affinity</a></li>
<li class="toctree-l4"><a class="reference internal" href="stubs/torchdr.LogAffinity.html">LogAffinity</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#spotlight-on-affinities-based-on-entropic-projections">Spotlight on affinities based on entropic projections</a></li>
<li class="toctree-l3"><a class="reference internal" href="#examples-using-entropicaffinity">Examples using <code class="docutils literal notranslate"><span class="pre">EntropicAffinity</span></code>:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#other-various-affinities">Other various affinities</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#dimensionality-reduction-modules">Dimensionality Reduction Modules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="stubs/torchdr.DRModule.html">DRModule</a></li>
<li class="toctree-l3"><a class="reference internal" href="stubs/torchdr.AffinityMatcher.html">AffinityMatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spectral-methods">Spectral methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="#neighbor-embedding">Neighbor Embedding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="all.html">API and Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="auto_examples/index.html">Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.contributing.html">How to Contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchdr.bibliography.html">Bibliography</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: white" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TorchDR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">User Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/torchdr.user_guide.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="module-torchdr">
<span id="user-guide"></span><span id="id1"></span><h1>User Guide<a class="headerlink" href="#module-torchdr" title="Link to this heading"></a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id9">Overview</a></p></li>
<li><p><a class="reference internal" href="#affinities" id="id10">Affinities</a></p></li>
<li><p><a class="reference internal" href="#dimensionality-reduction-modules" id="id11">Dimensionality Reduction Modules</a></p></li>
</ul>
</nav>
<section id="overview">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<section id="general-formulation-of-dimensionality-reduction">
<h3>General Formulation of Dimensionality Reduction<a class="headerlink" href="#general-formulation-of-dimensionality-reduction" title="Link to this heading"></a></h3>
<p>DR aims to construct a low-dimensional representation (or embedding) <span class="math notranslate nohighlight">\(\mathbf{Z} = (\mathbf{z}_1, ..., \mathbf{z}_n)^\top\)</span> of an input dataset <span class="math notranslate nohighlight">\(\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_n)^\top\)</span> that best preserves its geometry, encoded via a pairwise affinity matrix <span class="math notranslate nohighlight">\(\mathbf{A_X}\)</span>. To this end, DR methods optimize <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> such that a pairwise affinity matrix in the embedding space (denoted <span class="math notranslate nohighlight">\(\mathbf{A_Z}\)</span>) matches <span class="math notranslate nohighlight">\(\mathbf{A_X}\)</span>. This general problem is as follows</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{Z}} \: \mathcal{L}( \mathbf{A_X}, \mathbf{A_Z}) \quad \text{(DR)}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is typically the <span class="math notranslate nohighlight">\(\ell_2\)</span> or cross-entropy loss.
Each DR method is thus characterized by a triplet <span class="math notranslate nohighlight">\((\mathcal{L}, \mathbf{A_X}, \mathbf{A_Z})\)</span>.</p>
<p>TorchDR is structured around the above formulation <span class="math notranslate nohighlight">\(\text{(DR)}\)</span>.
Defining a DR algorithm solely requires providing an <a class="reference internal" href="stubs/torchdr.Affinity.html#torchdr.Affinity" title="torchdr.Affinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">Affinity</span></code></a> object for both input and embedding as well as a loss function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span>.</p>
<p>All modules follow the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> <span id="id2">[<a class="reference internal" href="torchdr.bibliography.html#id18" title="Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, and others. Scikit-learn: machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.">Pedregosa <em>et al.</em>, 2011</a>]</span> API and can be used in <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html">sklearn pipelines</a>.</p>
</section>
<section id="torch-gpu-support-and-automatic-differentiation">
<h3>Torch GPU support and automatic differentiation<a class="headerlink" href="#torch-gpu-support-and-automatic-differentiation" title="Link to this heading"></a></h3>
<p>TorchDR is built on top of <code class="docutils literal notranslate"><span class="pre">torch</span></code> <span id="id3">[<a class="reference internal" href="torchdr.bibliography.html#id17" title="Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, and others. Pytorch: an imperative style, high-performance deep learning library. Advances in neural information processing systems, 2019.">Paszke <em>et al.</em>, 2019</a>]</span>, offering GPU support and automatic differentiation. This foundation enables efficient computations and straightforward implementation of new DR methods.</p>
<p>To utilize GPU support, set <code class="xref py py-attr docutils literal notranslate"><span class="pre">device=&quot;cuda&quot;</span></code> when initializing any module. For CPU computations, set <code class="xref py py-attr docutils literal notranslate"><span class="pre">device=&quot;cpu&quot;</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DR particularly benefits from GPU acceleration as most computations, including affinity calculations and the DR objective, involve matrix reductions that are highly parallelizable.</p>
</div>
</section>
<section id="avoiding-memory-overflows-with-keops-symbolic-lazy-tensors">
<h3>Avoiding memory overflows with KeOps symbolic (lazy) tensors<a class="headerlink" href="#avoiding-memory-overflows-with-keops-symbolic-lazy-tensors" title="Link to this heading"></a></h3>
<p>Affinities incur a quadratic memory cost, which can be particularly problematic when dealing with large numbers of samples, especially when using GPUs.</p>
<p>To prevent memory overflows, TorchDR relies on <code class="docutils literal notranslate"><span class="pre">pykeops</span></code> <span id="id4">[<a class="reference internal" href="torchdr.bibliography.html#id16" title="Benjamin Charlier, Jean Feydy, Joan Alexis Glaunes, François-David Collin, and Ghislain Durif. Kernel operations on the gpu, with autodiff, without memory overflows. Journal of Machine Learning Research, 22(74):1–6, 2021.">Charlier <em>et al.</em>, 2021</a>]</span> lazy tensors. These tensors are expressed as mathematical formulas, evaluated directly on the data samples. This symbolic representation allows computations to be performed without storing the entire matrix in memory, thereby effectively eliminating any memory limitation.</p>
<a class="reference internal image-reference" href="_images/symbolic_matrix.svg"><img alt="_images/symbolic_matrix.svg" class="align-center" src="_images/symbolic_matrix.svg" style="width: 800px;" />
</a>
<p>The above figure is taken from <a class="reference external" href="https://github.com/getkeops/keops/blob/main/doc/_static/symbolic_matrix.svg">here</a>.</p>
<p>Set <code class="xref py py-attr docutils literal notranslate"><span class="pre">keops=True</span></code> as input to any module to use symbolic tensors. For small datasets, setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">keops=False</span></code> allows the computation of the full affinity matrix directly in memory.</p>
</section>
</section>
<section id="affinities">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Affinities</a><a class="headerlink" href="#affinities" title="Link to this heading"></a></h2>
<p>Affinities are the essential building blocks of dimensionality reduction methods.
TorchDR provides a wide range of affinities, including basic ones such as <a class="reference internal" href="gen_modules/torchdr.GaussianAffinity.html#torchdr.GaussianAffinity" title="torchdr.GaussianAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianAffinity</span></code></a>, <a class="reference internal" href="gen_modules/torchdr.StudentAffinity.html#torchdr.StudentAffinity" title="torchdr.StudentAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">StudentAffinity</span></code></a> and <a class="reference internal" href="gen_modules/torchdr.ScalarProductAffinity.html#torchdr.ScalarProductAffinity" title="torchdr.ScalarProductAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">ScalarProductAffinity</span></code></a>.</p>
<section id="base-structure">
<h3>Base structure<a class="headerlink" href="#base-structure" title="Link to this heading"></a></h3>
<p>Affinities inherit the structure of the following <a class="reference internal" href="stubs/torchdr.Affinity.html#torchdr.Affinity" title="torchdr.Affinity"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Affinity()</span></code></a> class.</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="stubs/torchdr.Affinity.html#torchdr.Affinity" title="torchdr.Affinity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchdr.Affinity</span></code></a></p></td>
<td><p>Base class for affinity matrices.</p></td>
</tr>
</tbody>
</table>
<p>If computations can be performed in log domain, the <a class="reference internal" href="stubs/torchdr.LogAffinity.html#torchdr.LogAffinity" title="torchdr.LogAffinity"><code class="xref py py-meth docutils literal notranslate"><span class="pre">LogAffinity()</span></code></a> class should be used.</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="stubs/torchdr.LogAffinity.html#torchdr.LogAffinity" title="torchdr.LogAffinity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchdr.LogAffinity</span></code></a></p></td>
<td><p>Base class for affinity matrices in log domain.</p></td>
</tr>
</tbody>
</table>
<p>Affinities are objects that can directly be called. The outputed affinity matrix is a <strong>square matrix of size (n, n)</strong> where n is the number of input samples.</p>
<p>Here is an example with the <a class="reference internal" href="gen_modules/torchdr.GaussianAffinity.html#torchdr.GaussianAffinity" title="torchdr.GaussianAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianAffinity</span></code></a>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">torchdr</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">affinity</span> <span class="o">=</span> <span class="n">torchdr</span><span class="o">.</span><span class="n">GaussianAffinity</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">affinity_matrix</span> <span class="o">=</span> <span class="n">affinity</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">affinity_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(100, 100)</span>
</pre></div>
</div>
</section>
<section id="spotlight-on-affinities-based-on-entropic-projections">
<h3>Spotlight on affinities based on entropic projections<a class="headerlink" href="#spotlight-on-affinities-based-on-entropic-projections" title="Link to this heading"></a></h3>
<p>A widely used family of affinities focuses on <strong>controlling the entropy</strong> of the affinity matrix. It is notably a crucial component of Neighbor-Embedding methods (see <a class="reference internal" href="#neighbor-embedding-section"><span class="std std-ref">Neighbor Embedding</span></a>).</p>
<p>These affinities are normalized such that each row sums to one, allowing the affinity matrix to be viewed as a <strong>Markov transition matrix</strong>. An <strong>adaptive bandwidth</strong> parameter then determines how the mass from each point spreads to its neighbors. The bandwidth is based on the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> hyperparameter which controls the <strong>number of effective neighbors</strong> for each point.</p>
<p>The resulting affinities can be viewed as a <strong>soft approximation of a k-nearest neighbor graph</strong>, where <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> takes the role of k. This allows for capturing more nuances than binary weights, as closer neighbors receive a higher weight compared to those farther away. Ultimately, <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> is an interpretable hyperparameter that governs the scale of dependencies represented in the affinity.</p>
<p>The following table outlines the aspects controlled by different formulations of entropic affinities. <strong>Marginal</strong> indicates whether each row of the affinity matrix has a controlled sum. <strong>Symmetry</strong> indicates whether the affinity matrix is symmetric. <strong>Entropy</strong> indicates whether each row of the affinity matrix has controlled entropy, dictated by the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> hyperparameter.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Affinity (associated DR method)</strong></p></th>
<th class="head"><p><strong>Marginal</strong></p></th>
<th class="head"><p><strong>Symmetry</strong></p></th>
<th class="head"><p><strong>Entropy</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.NormalizedGaussianAffinity.html#torchdr.NormalizedGaussianAffinity" title="torchdr.NormalizedGaussianAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">NormalizedGaussianAffinity</span></code></a></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.SinkhornAffinity.html#torchdr.SinkhornAffinity" title="torchdr.SinkhornAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SinkhornAffinity</span></code></a></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a></p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.SymmetricEntropicAffinity.html#torchdr.SymmetricEntropicAffinity" title="torchdr.SymmetricEntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SymmetricEntropicAffinity</span></code></a></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
<p>More details on these affinities can be found in the <a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/8b54ecd9823fff6d37e61ece8f87e534-Paper-Conference.pdf">SNEkhorn paper</a> <span id="id5">[<a class="reference internal" href="torchdr.bibliography.html#id4" title="Hugues Van Assel, Titouan Vayer, Rémi Flamary, and Nicolas Courty. Snekhorn: dimension reduction with symmetric entropic affinities. Advances in Neural Information Processing Systems, 2024.">Van Assel <em>et al.</em>, 2024</a>]</span>.</p>
</section>
<section id="examples-using-entropicaffinity">
<h3>Examples using <code class="docutils literal notranslate"><span class="pre">EntropicAffinity</span></code>:<a class="headerlink" href="#examples-using-entropicaffinity" title="Link to this heading"></a></h3>
<div class="sphx-glr-thumbnails"><div class="sphx-glr-thumbcontainer" tooltip="We show the adaptivity property of entropic affinities on a toy simulated dataset with heteroscedastic noise."><img alt="" src="_images/sphx_glr_demo_ea_adaptivity_thumb.png" />
<p><a class="reference internal" href="auto_examples/affinities/demo_ea_adaptivity.html#sphx-glr-auto-examples-affinities-demo-ea-adaptivity-py"><span class="std std-ref">Entropic Affinities can adapt to varying noise levels</span></a></p>
  <div class="sphx-glr-thumbnail-title">Entropic Affinities can adapt to varying noise levels</div>
</div><div class="sphx-glr-thumbcontainer" tooltip="We illustrate the basic usage of TorchDR with different neighbor embedding methods on the SNARE-seq gene expression dataset with given cell type labels."><img alt="" src="_images/sphx_glr_demo_ne_methods_affinity_matcher_thumb.png" />
<p><a class="reference internal" href="auto_examples/basics/demo_ne_methods_affinity_matcher.html#sphx-glr-auto-examples-basics-demo-ne-methods-affinity-matcher-py"><span class="std std-ref">Neighbor Embedding on genomics &amp; equivalent affinity matcher formulation</span></a></p>
  <div class="sphx-glr-thumbnail-title">Neighbor Embedding on genomics & equivalent affinity matcher formulation</div>
</div></div></section>
<section id="other-various-affinities">
<h3>Other various affinities<a class="headerlink" href="#other-various-affinities" title="Link to this heading"></a></h3>
<p>TorchDR features other affinities that can be used in various contexts.</p>
<p>For instance, the UMAP <span id="id6">[<a class="reference internal" href="torchdr.bibliography.html#id9" title="Leland McInnes, John Healy, and James Melville. Umap: uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018.">McInnes <em>et al.</em>, 2018</a>]</span> algorithm relies on the affinities <a class="reference internal" href="gen_modules/torchdr.UMAPAffinityIn.html#torchdr.UMAPAffinityIn" title="torchdr.UMAPAffinityIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityIn</span></code></a> for the input data and <a class="reference internal" href="gen_modules/torchdr.UMAPAffinityOut.html#torchdr.UMAPAffinityOut" title="torchdr.UMAPAffinityOut"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityOut</span></code></a> in the embedding space. <a class="reference internal" href="gen_modules/torchdr.UMAPAffinityIn.html#torchdr.UMAPAffinityIn" title="torchdr.UMAPAffinityIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityIn</span></code></a> follows a similar construction as entropic affinities to ensure a constant number of effective neighbors, with <code class="xref py py-attr docutils literal notranslate"><span class="pre">n_neighbors</span></code> playing the role of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">perplexity</span></code> hyperparameter.</p>
<p>Another example is the doubly stochastic normalization of a base affinity under the <span class="math notranslate nohighlight">\(\ell_2\)</span> geometry that has recently been proposed for DR <span id="id7">[<a class="reference internal" href="torchdr.bibliography.html#id10" title="Stephen Zhang, Gilles Mordant, Tetsuya Matsumoto, and Geoffrey Schiebinger. Manifold learning with sparse regularised optimal transport. arXiv preprint arXiv:2307.09816, 2023.">Zhang <em>et al.</em>, 2023</a>]</span>. This method is analogous to <a class="reference internal" href="gen_modules/torchdr.SinkhornAffinity.html#torchdr.SinkhornAffinity" title="torchdr.SinkhornAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SinkhornAffinity</span></code></a> where the Shannon entropy is replaced by the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm to recover a sparse affinity.
It is available at <a class="reference internal" href="gen_modules/torchdr.DoublyStochasticQuadraticAffinity.html#torchdr.DoublyStochasticQuadraticAffinity" title="torchdr.DoublyStochasticQuadraticAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">DoublyStochasticQuadraticAffinity</span></code></a>.</p>
</section>
</section>
<section id="dimensionality-reduction-modules">
<h2><a class="toc-backref" href="#id11" role="doc-backlink">Dimensionality Reduction Modules</a><a class="headerlink" href="#dimensionality-reduction-modules" title="Link to this heading"></a></h2>
<p>TorchDR provides a wide range of dimensionality reduction (DR) methods. All DR estimators inherit the structure of the <a class="reference internal" href="stubs/torchdr.DRModule.html#torchdr.DRModule" title="torchdr.DRModule"><code class="xref py py-meth docutils literal notranslate"><span class="pre">DRModule()</span></code></a> class:</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="stubs/torchdr.DRModule.html#torchdr.DRModule" title="torchdr.DRModule"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchdr.DRModule</span></code></a></p></td>
<td><p>Base class for DR methods.</p></td>
</tr>
</tbody>
</table>
<p>They are <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code> and <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.TransformerMixin</span></code> classes which can be called with the <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code> method.</p>
<p>Outside of <a class="reference internal" href="#spectral-section"><span class="std std-ref">Spectral methods</span></a>, a closed-form solution to the DR problem is typically not available. The problem can then be solved using <a class="reference external" href="https://pytorch.org/docs/stable/optim.html">gradient-based optimizers</a>.</p>
<p>The following classes serve as parent classes for this approach, requiring the user to provide affinity objects for the input and output spaces, referred to as <code class="xref py py-attr docutils literal notranslate"><span class="pre">affinity_in</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">affinity_out</span></code>.</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="stubs/torchdr.AffinityMatcher.html#torchdr.AffinityMatcher" title="torchdr.AffinityMatcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">torchdr.AffinityMatcher</span></code></a></p></td>
<td><p>Perform dimensionality reduction by matching two affinity matrices.</p></td>
</tr>
</tbody>
</table>
<p>In what follows we briefly present two families of DR algorithms: neighbor embedding methods and spectral methods.</p>
<section id="spectral-methods">
<span id="spectral-section"></span><h3>Spectral methods<a class="headerlink" href="#spectral-methods" title="Link to this heading"></a></h3>
<p>Spectral methods correspond to choosing the scalar product affinity <span class="math notranslate nohighlight">\([\mathbf{A_X}]_{ij} = \langle \mathbf{z}_i, \mathbf{z}_j \rangle\)</span> for the embeddings and the <span class="math notranslate nohighlight">\(\ell_2\)</span> loss.</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{Z}} \: \sum_{ij} ( [\mathbf{A_X}]_{ij} - \langle \mathbf{z}_i, \mathbf{z}_j \rangle )^{2}\]</div>
<p>When <span class="math notranslate nohighlight">\(\mathbf{A_X}\)</span> is positive semi-definite, this problem is commonly known as kernel Principal Component Analysis <span id="id8">[<a class="reference internal" href="torchdr.bibliography.html#id11" title="Jihun Ham, Daniel D Lee, Sebastian Mika, and Bernhard Schölkopf. A kernel view of the dimensionality reduction of manifolds. In Proceedings of the twenty-first international conference on Machine learning, 47. 2004.">Ham <em>et al.</em>, 2004</a>]</span> and an optimal solution is given by</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z}^{\star} = (\sqrt{\lambda_1} \mathbf{v}_1, ..., \sqrt{\lambda_d} \mathbf{v}_d)^\top\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_1, ..., \lambda_d\)</span> are the largest eigenvalues of the centered kernel matrix <span class="math notranslate nohighlight">\(\mathbf{A_X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}_1, ..., \mathbf{v}_d\)</span> are the corresponding eigenvectors.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>PCA (available at <a class="reference internal" href="gen_modules/torchdr.PCA.html#torchdr.PCA" title="torchdr.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">torchdr.PCA</span></code></a>) corresponds to choosing <span class="math notranslate nohighlight">\([\mathbf{A_X}]_{ij} = \langle \mathbf{x}_i, \mathbf{x}_j \rangle\)</span>.</p>
</div>
</section>
<section id="neighbor-embedding">
<span id="neighbor-embedding-section"></span><h3>Neighbor Embedding<a class="headerlink" href="#neighbor-embedding" title="Link to this heading"></a></h3>
<p>TorchDR aims to implement most popular <strong>neighbor embedding (NE)</strong> algorithms.
In these methods, <span class="math notranslate nohighlight">\(\mathbf{A_X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{A_Z}\)</span> can be viewed as <strong>soft neighborhood graphs</strong>, hence the term <em>neighbor embedding</em>.</p>
<p>NE objectives share a common structure: they aim to <strong>minimize</strong> the <strong>weighted sum</strong> of an <strong>attractive term</strong> and a <strong>repulsive term</strong>. Interestingly, the <strong>attractive term</strong> is often the <strong>cross-entropy</strong> between the input and output affinities. Additionally, the <strong>repulsive term</strong> is typically a <strong>function of the output affinities only</strong>. Thus, the NE problem can be formulated as the following minimization problem:</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{Z}} \: - \sum_{ij} [\mathbf{A_X}]_{ij} \log [\mathbf{A_Z}]_{ij} + \gamma \mathcal{L}_{\mathrm{rep}}(\mathbf{A_Z}) \:.\]</div>
<p>In the above, <span class="math notranslate nohighlight">\(\mathcal{L}_{\mathrm{rep}}(\mathbf{A_Z})\)</span> represents the repulsive part of the loss function while <span class="math notranslate nohighlight">\(\gamma\)</span> is a hyperparameter that controls the balance between attraction and repulsion. The latter is called <code class="xref py py-attr docutils literal notranslate"><span class="pre">coeff_repulsion</span></code> in TorchDR.</p>
<p>Many NE methods can be represented within this framework. The following table summarizes the ones implemented in TorchDR, detailing their respective repulsive loss function, as well as their input and output affinities.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>Method</strong></p></th>
<th class="head"><p><strong>Repulsive term</strong> <span class="math notranslate nohighlight">\(\mathcal{L}_{\mathrm{rep}}\)</span></p></th>
<th class="head"><p><strong>Affinity input</strong> <span class="math notranslate nohighlight">\(\mathbf{A_X}\)</span></p></th>
<th class="head"><p><strong>Affinity output</strong> <span class="math notranslate nohighlight">\(\mathbf{A_Z}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.SNE.html#torchdr.SNE" title="torchdr.SNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">SNE</span></code></a></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum_{i} \log(\sum_j [\mathbf{A_Z}]_{ij})\)</span></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.GaussianAffinity.html#torchdr.GaussianAffinity" title="torchdr.GaussianAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">GaussianAffinity</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.TSNE.html#torchdr.TSNE" title="torchdr.TSNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNE</span></code></a></p></td>
<td><p><span class="math notranslate nohighlight">\(\log(\sum_{ij} [\mathbf{A_Z}]_{ij})\)</span></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.StudentAffinity.html#torchdr.StudentAffinity" title="torchdr.StudentAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">StudentAffinity</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.TSNEkhorn.html#torchdr.TSNEkhorn" title="torchdr.TSNEkhorn"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNEkhorn</span></code></a></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum_{ij} [\mathbf{A_Z}]_{ij}\)</span></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.SymmetricEntropicAffinity.html#torchdr.SymmetricEntropicAffinity" title="torchdr.SymmetricEntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SymmetricEntropicAffinity</span></code></a></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.SinkhornAffinity.html#torchdr.SinkhornAffinity" title="torchdr.SinkhornAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">SinkhornAffinity(base_kernel=&quot;student&quot;)</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.InfoTSNE.html#torchdr.InfoTSNE" title="torchdr.InfoTSNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">InfoTSNE</span></code></a></p></td>
<td><p><span class="math notranslate nohighlight">\(\sum_i \log(\sum_{j \in N(i)} [\mathbf{A_Z}]_{ij})\)</span></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.StudentAffinity.html#torchdr.StudentAffinity" title="torchdr.StudentAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">StudentAffinity</span></code></a></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="gen_modules/torchdr.UMAP.html#torchdr.UMAP" title="torchdr.UMAP"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAP</span></code></a></p></td>
<td><p><span class="math notranslate nohighlight">\(- \sum_{i, j \in N(i)} \log (1 - [\mathbf{A_Z}]_{ij})\)</span></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.UMAPAffinityIn.html#torchdr.UMAPAffinityIn" title="torchdr.UMAPAffinityIn"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityIn</span></code></a></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.UMAPAffinityOut.html#torchdr.UMAPAffinityOut" title="torchdr.UMAPAffinityOut"><code class="xref py py-class docutils literal notranslate"><span class="pre">UMAPAffinityOut</span></code></a></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="gen_modules/torchdr.LargeVis.html#torchdr.LargeVis" title="torchdr.LargeVis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LargeVis</span></code></a></p></td>
<td><p><span class="math notranslate nohighlight">\(- \sum_{i, j \in N(i)} \log (1 - [\mathbf{A_Z}]_{ij})\)</span></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.EntropicAffinity.html#torchdr.EntropicAffinity" title="torchdr.EntropicAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">EntropicAffinity</span></code></a></p></td>
<td><p><a class="reference internal" href="gen_modules/torchdr.StudentAffinity.html#torchdr.StudentAffinity" title="torchdr.StudentAffinity"><code class="xref py py-class docutils literal notranslate"><span class="pre">StudentAffinity</span></code></a></p></td>
</tr>
</tbody>
</table>
<p>In the above table, <span class="math notranslate nohighlight">\(N(i)\)</span> denotes the set of negative samples
for point <span class="math notranslate nohighlight">\(i\)</span>. They are usually sampled uniformly at random from the dataset.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchdr.quick_start.html" class="btn btn-neutral float-left" title="Quick Start Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="stubs/torchdr.Affinity.html" class="btn btn-neutral float-right" title="Affinity" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, TorchDR team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>