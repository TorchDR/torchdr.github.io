{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Neighbor Embedding on genomics & equivalent affinity matcher formulation\n\nWe illustrate the basic usage of TorchDR with different neighbor embedding methods\non the SNARE-seq gene expression dataset with given cell type labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Titouan Vayer <titouan.vayer@inria.fr>\n#         Hugues Van Assel <vanasselhugues@gmail.com>\n#\n# License: BSD 3-Clause License\n\nimport urllib.request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torchdr import (\n    SNE,\n    TSNE,\n    UMAP,\n    AffinityMatcher,\n    EntropicAffinity,\n    NormalizedGaussianAffinity,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the SNARE-seq dataset (gene expression) with cell type labels\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_numpy_from_url(url, delimiter=\"\\t\"):\n    response = urllib.request.urlopen(url)\n    data = response.read().decode(\"utf-8\")\n    data = data.split(\"\\n\")\n    data = [row.split(delimiter) for row in data if row]\n    numpy_array = np.array(data, dtype=float)\n    return numpy_array\n\n\nurl_x = \"https://rsinghlab.github.io/SCOT/data/snare_rna.txt\"\nX = load_numpy_from_url(url_x)\n\nurl_y = \"https://rsinghlab.github.io/SCOT/data/SNAREseq_types.txt\"\nY = load_numpy_from_url(url_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run neighbor embedding methods\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "params = {\n    \"optimizer\": \"Adam\",\n    \"optimizer_kwargs\": None,\n    \"max_iter\": 100,\n    \"lr\": 1e0,\n}\n\nsne = SNE(early_exaggeration_coeff=1, **params)\n\numap = UMAP(early_exaggeration_coeff=1, **params)\n\ntsne = TSNE(early_exaggeration_coeff=1, **params)\n\nall_methods = {\n    \"TSNE\": tsne,\n    \"SNE\": sne,\n    \"UMAP\": umap,\n}\n\nfor method_name, method in all_methods.items():\n    print(f\"--- Computing {method_name} ---\")\n    method.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the different embeddings\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(12, 4))\n\nfor i, (method_name, method) in enumerate(all_methods.items()):\n    ax = fig.add_subplot(1, 3, i + 1)\n    emb = method.embedding_.detach().numpy()  # get the embedding\n    ax.scatter(emb[:, 0], emb[:, 1], c=Y, s=10)\n    ax.set_title(f\"{method_name}\", fontsize=24)\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using AffinityMatcher\n\nWe can reproduce the same embeddings using the\nclass :class:`torchdr.AffinityMatcher`. The latter takes as input\ntwo affinities and minimize a certain matching loss between them.\n\nTo reproduce the SNE algorithm\nwe can match, via the cross entropy loss,\na :class:`torchdr.EntropicAffinity` with a\n:class:`torchdr.NormalizedGaussianAffinity`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sne_affinity_matcher = AffinityMatcher(\n    n_components=2,\n    # SNE matches an EntropicAffinity\n    affinity_in=EntropicAffinity(sparsity=False),\n    # with a Gaussian kernel normalized by row\n    affinity_out=NormalizedGaussianAffinity(normalization_dim=1),\n    loss_fn=\"cross_entropy_loss\",  # and the cross_entropy loss\n    **params,\n)\nsne_affinity_matcher.fit(X)\n\nfig = plt.figure(figsize=(8, 4))\ntwo_sne_dict = {\"SNE\": sne, \"SNE (with affinity matcher)\": sne_affinity_matcher}\nfor i, (method_name, method) in enumerate(two_sne_dict.items()):\n    ax = fig.add_subplot(1, 2, i + 1)\n    emb = method.embedding_.detach().numpy()  # get the embedding\n    ax.scatter(emb[:, 0], emb[:, 1], c=Y, s=10)\n    ax.set_title(f\"{method_name}\", fontsize=15)\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## On the efficiency of using the torchdr API rather than AffinityMatcher directly\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Calling :class:`torchdr.SNE` enables to leverage sparsity and therefore\n    significantly reduces the computational cost of the algorithm compared to\n    using :class:`torchdr.AffinityMatcher` with the corresponding affinities.\n    In TorchDR, it is therefore recommended to use the specific class associated\n    with the desired algorithm when available.</p></div>\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}