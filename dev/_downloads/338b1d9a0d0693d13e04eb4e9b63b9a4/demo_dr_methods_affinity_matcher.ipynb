{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparison of different DR methods and the use of affinity matcher\n\nWe illustrate the basic usage of TorchDR with different DR methods\non the swiss roll dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\n\nfrom torchdr import AffinityMatcher, SNE, UMAP, TSNE\nfrom torchdr.affinity import EntropicAffinity, NormalizedGaussianAffinity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load toy images\n\nFirst, let's load 5 classes of the digits dataset from sklearn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\nn_samples = 500\nX, t = make_swiss_roll(n_samples=n_samples, noise=0.1, random_state=0)\n\ninit_embedding = torch.normal(0, 1, size=(\n                n_samples, 2), dtype=torch.double)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the different embedding\n\nTune the different hyperparameters for better results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "perplexity = 30\nlr = 1e-1\noptim_params = {\n    'init': init_embedding,\n    'early_exaggeration_iter': 0,\n    'optimizer': 'Adam',\n    'optimizer_kwargs': None,\n    'early_exaggeration': 1.0,\n    'max_iter': 100\n    }\n\nsne = SNE(n_components=2,\n          perplexity=perplexity,\n          lr=lr, **optim_params)\n\numap = UMAP(n_neighbors=perplexity,\n            n_components=2,\n            lr=lr, **optim_params)\n\ntsne = TSNE(n_components=2,\n            perplexity=perplexity,\n            lr=lr, **optim_params)\n\nall_methods = {\n    'TSNE': tsne,\n    'SNE': sne,\n    'UMAP': umap,\n}\n\nfor method_name, method in all_methods.items():\n    print('--- Computing {} ---'.format(method_name))\n    method.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the different embeddings\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(15, 4))\nfs = 24\nax = fig.add_subplot(1, 4, 1, projection='3d')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, s=20)\nax.set_title('Swiss Roll in ambient space', font='Times New Roman', fontsize=fs)\nax.view_init(azim=-66, elev=12)\n\nfor i, (method_name, method) in enumerate(all_methods.items()):\n    ax = fig.add_subplot(1, 4, i+2)\n    emb = method.embedding_.detach().numpy()  # get the embedding\n    ax.scatter(emb[:, 0], emb[:, 1], c=t, s=20)\n    ax.set_title('{0}'.format(method_name), font='Times New Roman', fontsize=fs)\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using AffinityMatcher\n\nWe can reproduce the same kind of results using the\nflexible class AffinityMatcher\n:class:`torchdr.AffinityMatcher`. It take as input\ntwo affinities and minimize a certain matching loss\nbetween them. To reproduce the SNE algorithm\nwe can match with the cross entropy loss\nan EntropicAffinity\n:class:`torchdr.affinity.EntropicAffinity` with given\nperplexity and a NormalizedGaussianAffinity\n:class:`torchdr.affinity.NormalizedGaussianAffinity`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sne_affinity_matcher = AffinityMatcher(\n    n_components=2,\n    # SNE matches an EntropicAffinity\n    affinity_in=EntropicAffinity(perplexity=perplexity),\n    # with a Gaussian kernel normalized by row\n    affinity_out=NormalizedGaussianAffinity(normalization_dim=1),\n    loss_fn=\"cross_entropy_loss\",  # and the cross_entropy loss\n    init=init_embedding,\n    max_iter=200,\n    lr=lr\n)\nsne_affinity_matcher.fit(X)\n\nfig = plt.figure(figsize=(10, 4))\nfs = 24\ntwo_sne_dict = {'SNE': sne, 'SNE (with affinity matcher)': sne_affinity_matcher}\nfor i, (method_name, method) in enumerate(two_sne_dict.items()):\n    ax = fig.add_subplot(1, 2, i+1)\n    emb = method.embedding_.detach().numpy()  # get the embedding\n    ax.scatter(emb[:, 0], emb[:, 1], c=t, s=20)\n    ax.set_title('{0}'.format(method_name), font='Times New Roman', fontsize=fs)\n    ax.set_xticks([])\n    ax.set_yticks([])\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}