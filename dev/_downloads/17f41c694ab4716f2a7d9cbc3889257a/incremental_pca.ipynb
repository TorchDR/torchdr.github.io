{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Incremental PCA on GPU\n\nThis example demonstrates how to use the `IncrementalPCA` class on GPU.\nWe compare the memory usage and time taken to fit the model with the regular\n`PCA` class on GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport gc\nimport torch\n\nfrom torchdr import IncrementalPCA, PCA\n\n# Choose the GPU device if available.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type != \"cuda\":\n    raise RuntimeError(\"This example requires a CUDA-capable GPU.\")\n\n# Generate a large random dataset on CPU.\nn_samples = 100_000\nn_features = 500\nX = torch.randn(n_samples, n_features)  # Stored on CPU\n\n# --------------------------\n# Incremental PCA on GPU\n# --------------------------\n# IncrementalPCA is designed so that only each batch is moved to the GPU.\n# Set n_components=50 and choose an appropriate batch_size.\nipca = IncrementalPCA(n_components=50, batch_size=1024, device=device)\n\n# Reset GPU memory stats before fitting.\ntorch.cuda.reset_peak_memory_stats(device)\n\nstart = time.time()\nipca.fit(X, check_input=True)\nipca_time = time.time() - start\n\n# Get the peak GPU memory allocated (in bytes).\nipca_peak_mem = torch.cuda.max_memory_allocated(device)\nprint(\n    f\"Incremental PCA on GPU:\\n\"\n    f\"  Time: {ipca_time:.2f} sec, \"\n    f\"Peak GPU memory: {ipca_peak_mem / 1024**2:.2f} MB\"\n)\n\n# Clean up and free GPU memory.\ndel ipca\ntorch.cuda.empty_cache()\ngc.collect()\n\n# --------------------------\n# Regular PCA on GPU\n# --------------------------\n# Regular PCA (from torchdr) will typically move the full dataset to GPU.\npca = PCA(n_components=50, device=device)\n\n# Reset GPU memory stats before fitting.\ntorch.cuda.reset_peak_memory_stats(device)\n\nstart = time.time()\n# Here we assume that pca.fit will move the full dataset to the GPU.\npca.fit(X)\npca_time = time.time() - start\n\n# Get the peak GPU memory allocated (in bytes).\npca_peak_mem = torch.cuda.max_memory_allocated(device)\nprint(\n    f\"\\nRegular PCA on GPU:\\n\"\n    f\"  Time: {pca_time:.2f} sec, \"\n    f\"Peak GPU memory: {pca_peak_mem / 1024**2:.2f} MB\"\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}