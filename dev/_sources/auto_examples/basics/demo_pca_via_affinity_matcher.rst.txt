
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/basics/demo_pca_via_affinity_matcher.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_basics_demo_pca_via_affinity_matcher.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_basics_demo_pca_via_affinity_matcher.py:


PCA via SVD and via AffinityMatcher
===================================

We show how to compute a PCA embedding using the closed form
and using the AffinityMatcher class. Both approaches lead to the same solution.

.. GENERATED FROM PYTHON SOURCE LINES 9-16

.. code-block:: Python


    import matplotlib.pyplot as plt
    from sklearn.datasets import load_digits

    from torchdr.spectral import PCA
    from torchdr import AffinityMatcher, ScalarProductAffinity








.. GENERATED FROM PYTHON SOURCE LINES 17-21

Load toy images
---------------

First, let's load 5 classes of the digits dataset from sklearn.

.. GENERATED FROM PYTHON SOURCE LINES 21-25

.. code-block:: Python


    digits = load_digits(n_class=5)
    X = digits.data








.. GENERATED FROM PYTHON SOURCE LINES 26-32

PCA via SVD
-----------

Let us perform PCA using the closed form solution given by the
Singular Value Decomposition (SVD).
In ``Torchdr``, it is available at :class:`torchdr.PCA`.

.. GENERATED FROM PYTHON SOURCE LINES 32-41

.. code-block:: Python


    Z_svd = PCA(n_components=2).fit_transform(X)

    plt.figure()
    plt.scatter(Z_svd[:, 0], Z_svd[:, 1], c=digits.target)
    plt.title("PCA via SVD")
    plt.show()





.. image-sg:: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_001.png
   :alt: PCA via SVD
   :srcset: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 42-51

PCA via AffinityMatcher
-----------------------

Now, let us perform PCA using the AffinityMatcher class
:class:`torchdr.AffinityMatcher`
as well as the scalar product affinity
:class:`torchdr.ScalarProductAffinity`
for both input data and embeddings,
and the square loss as global objective.

.. GENERATED FROM PYTHON SOURCE LINES 51-70

.. code-block:: Python


    model = AffinityMatcher(
        n_components=2,
        affinity_in=ScalarProductAffinity(centering=True),
        affinity_out=ScalarProductAffinity(),
        loss_fn="square_loss",
        init="normal",
        lr=1e1,
        max_iter=50,
        keops=False,
    )
    Z_am = model.fit_transform(X)

    plt.figure()
    plt.scatter(Z_am[:, 0], Z_am[:, 1], c=digits.target)
    plt.title("PCA via AffinityMatcher")
    plt.show()





.. image-sg:: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_002.png
   :alt: PCA via AffinityMatcher
   :srcset: /auto_examples/basics/images/sphx_glr_demo_pca_via_affinity_matcher_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      0%|          | 0/50 [00:00<?, ?it/s][KeOps] Generating code for Sum_Reduction reduction (with parameters 0) of formula (a|b-c|d)**2 with a=Var(0,64,0), b=Var(1,64,1), c=Var(2,2,0), d=Var(3,2,1) ... OK
    [pyKeOps] Compiling pykeops cpp 4a450c041c module ... OK
    [KeOps] Generating code for Sum_Reduction reduction (with parameters 0) of formula -2*((e*d)*(a|b-c|d)) with a=Var(0,64,0), b=Var(1,64,1), c=Var(2,2,0), d=Var(3,2,1), e=Var(4,1,0) ... OK
    [pyKeOps] Compiling pykeops cpp 400563ca71 module ... OK
    [KeOps] Generating code for Sum_Reduction reduction (with parameters 1) of formula -2*((e*c)*(a|b-c|d)) with a=Var(0,64,0), b=Var(1,64,1), c=Var(2,2,0), d=Var(3,2,1), e=Var(4,1,0) ... OK
    [pyKeOps] Compiling pykeops cpp 8a04c810cd module ... OK
    /home/circleci/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
      warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
    Loss : 1.35e+11:   0%|          | 0/50 [00:14<?, ?it/s]    Loss : 1.35e+11:   2%|▏         | 1/50 [00:14<12:00, 14.71s/it]    Loss : 9.72e+10:   2%|▏         | 1/50 [00:15<12:00, 14.71s/it]    Loss : 9.72e+10:   4%|▍         | 2/50 [00:15<04:59,  6.24s/it]    Loss : 9.94e+10:   4%|▍         | 2/50 [00:15<04:59,  6.24s/it]    Loss : 9.94e+10:   6%|▌         | 3/50 [00:15<02:47,  3.57s/it]    Loss : 7.84e+10:   6%|▌         | 3/50 [00:15<02:47,  3.57s/it]    Loss : 7.84e+10:   8%|▊         | 4/50 [00:15<01:46,  2.32s/it]    Loss : 8.24e+10:   8%|▊         | 4/50 [00:16<01:46,  2.32s/it]    Loss : 8.24e+10:  10%|█         | 5/50 [00:16<01:11,  1.59s/it]    Loss : 8.05e+10:  10%|█         | 5/50 [00:16<01:11,  1.59s/it]    Loss : 8.05e+10:  12%|█▏        | 6/50 [00:16<00:52,  1.19s/it]    Loss : 6.82e+10:  12%|█▏        | 6/50 [00:16<00:52,  1.19s/it]    Loss : 6.82e+10:  14%|█▍        | 7/50 [00:16<00:38,  1.11it/s]    Loss : 5.69e+10:  14%|█▍        | 7/50 [00:17<00:38,  1.11it/s]    Loss : 5.69e+10:  16%|█▌        | 8/50 [00:17<00:31,  1.35it/s]    Loss : 5.29e+10:  16%|█▌        | 8/50 [00:17<00:31,  1.35it/s]    Loss : 5.29e+10:  18%|█▊        | 9/50 [00:17<00:24,  1.66it/s]    Loss : 4.92e+10:  18%|█▊        | 9/50 [00:17<00:24,  1.66it/s]    Loss : 4.92e+10:  20%|██        | 10/50 [00:17<00:21,  1.86it/s]    Loss : 4.66e+10:  20%|██        | 10/50 [00:18<00:21,  1.86it/s]    Loss : 4.66e+10:  22%|██▏       | 11/50 [00:18<00:19,  2.01it/s]    Loss : 4.52e+10:  22%|██▏       | 11/50 [00:18<00:19,  2.01it/s]    Loss : 4.52e+10:  24%|██▍       | 12/50 [00:18<00:16,  2.29it/s]    Loss : 4.40e+10:  24%|██▍       | 12/50 [00:19<00:16,  2.29it/s]    Loss : 4.40e+10:  26%|██▌       | 13/50 [00:19<00:15,  2.36it/s]    Loss : 4.27e+10:  26%|██▌       | 13/50 [00:19<00:15,  2.36it/s]    Loss : 4.27e+10:  28%|██▊       | 14/50 [00:19<00:13,  2.57it/s]    Loss : 4.15e+10:  28%|██▊       | 14/50 [00:19<00:13,  2.57it/s]    Loss : 4.15e+10:  30%|███       | 15/50 [00:19<00:13,  2.56it/s]    Loss : 4.09e+10:  30%|███       | 15/50 [00:20<00:13,  2.56it/s]    Loss : 4.09e+10:  32%|███▏      | 16/50 [00:20<00:12,  2.75it/s]    Loss : 4.08e+10:  32%|███▏      | 16/50 [00:20<00:12,  2.75it/s]    Loss : 4.08e+10:  34%|███▍      | 17/50 [00:20<00:12,  2.67it/s]    Loss : 4.06e+10:  34%|███▍      | 17/50 [00:20<00:12,  2.67it/s]    Loss : 4.06e+10:  36%|███▌      | 18/50 [00:20<00:12,  2.62it/s]    Loss : 4.06e+10:  36%|███▌      | 18/50 [00:21<00:12,  2.62it/s]    Loss : 4.06e+10:  38%|███▊      | 19/50 [00:21<00:12,  2.58it/s]    Loss : 4.04e+10:  38%|███▊      | 19/50 [00:21<00:12,  2.58it/s]    Loss : 4.04e+10:  40%|████      | 20/50 [00:21<00:10,  2.77it/s]    Loss : 3.96e+10:  40%|████      | 20/50 [00:21<00:10,  2.77it/s]    Loss : 3.96e+10:  42%|████▏     | 21/50 [00:21<00:10,  2.68it/s]    Loss : 3.83e+10:  42%|████▏     | 21/50 [00:22<00:10,  2.68it/s]    Loss : 3.83e+10:  44%|████▍     | 22/50 [00:22<00:10,  2.63it/s]    Loss : 3.70e+10:  44%|████▍     | 22/50 [00:22<00:10,  2.63it/s]    Loss : 3.70e+10:  46%|████▌     | 23/50 [00:22<00:09,  2.79it/s]    Loss : 3.61e+10:  46%|████▌     | 23/50 [00:23<00:09,  2.79it/s]    Loss : 3.61e+10:  48%|████▊     | 24/50 [00:23<00:10,  2.50it/s]    Loss : 3.55e+10:  48%|████▊     | 24/50 [00:23<00:10,  2.50it/s]    Loss : 3.55e+10:  50%|█████     | 25/50 [00:23<00:09,  2.70it/s]    Loss : 3.51e+10:  50%|█████     | 25/50 [00:23<00:09,  2.70it/s]    Loss : 3.51e+10:  52%|█████▏    | 26/50 [00:23<00:09,  2.65it/s]    Loss : 3.53e+10:  52%|█████▏    | 26/50 [00:24<00:09,  2.65it/s]    Loss : 3.53e+10:  54%|█████▍    | 27/50 [00:24<00:08,  2.80it/s]    Loss : 3.55e+10:  54%|█████▍    | 27/50 [00:24<00:08,  2.80it/s]    Loss : 3.55e+10:  56%|█████▌    | 28/50 [00:24<00:08,  2.70it/s]    Loss : 3.57e+10:  56%|█████▌    | 28/50 [00:24<00:08,  2.70it/s]    Loss : 3.57e+10:  58%|█████▊    | 29/50 [00:24<00:07,  2.65it/s]    Loss : 3.56e+10:  58%|█████▊    | 29/50 [00:25<00:07,  2.65it/s]    Loss : 3.56e+10:  60%|██████    | 30/50 [00:25<00:07,  2.61it/s]    Loss : 3.53e+10:  60%|██████    | 30/50 [00:25<00:07,  2.61it/s]    Loss : 3.53e+10:  62%|██████▏   | 31/50 [00:25<00:06,  2.77it/s]    Loss : 3.49e+10:  62%|██████▏   | 31/50 [00:26<00:06,  2.77it/s]    Loss : 3.49e+10:  64%|██████▍   | 32/50 [00:26<00:07,  2.50it/s]    Loss : 3.45e+10:  64%|██████▍   | 32/50 [00:26<00:07,  2.50it/s]    Loss : 3.45e+10:  66%|██████▌   | 33/50 [00:26<00:06,  2.49it/s]    Loss : 3.42e+10:  66%|██████▌   | 33/50 [00:26<00:06,  2.49it/s]    Loss : 3.42e+10:  68%|██████▊   | 34/50 [00:26<00:05,  2.69it/s]    Loss : 3.40e+10:  68%|██████▊   | 34/50 [00:27<00:05,  2.69it/s]    Loss : 3.40e+10:  70%|███████   | 35/50 [00:27<00:05,  2.65it/s]    Loss : 3.39e+10:  70%|███████   | 35/50 [00:27<00:05,  2.65it/s]    Loss : 3.39e+10:  72%|███████▏  | 36/50 [00:27<00:04,  2.82it/s]    Loss : 3.39e+10:  72%|███████▏  | 36/50 [00:27<00:04,  2.82it/s]    Loss : 3.39e+10:  74%|███████▍  | 37/50 [00:27<00:04,  2.72it/s]    Loss : 3.38e+10:  74%|███████▍  | 37/50 [00:28<00:04,  2.72it/s]    Loss : 3.38e+10:  76%|███████▌  | 38/50 [00:28<00:04,  2.84it/s]    Loss : 3.38e+10:  76%|███████▌  | 38/50 [00:28<00:04,  2.84it/s]    Loss : 3.38e+10:  78%|███████▊  | 39/50 [00:28<00:03,  2.76it/s]    Loss : 3.37e+10:  78%|███████▊  | 39/50 [00:28<00:03,  2.76it/s]    Loss : 3.37e+10:  80%|████████  | 40/50 [00:28<00:03,  2.90it/s]    Loss : 3.37e+10:  80%|████████  | 40/50 [00:29<00:03,  2.90it/s]    Loss : 3.37e+10:  82%|████████▏ | 41/50 [00:29<00:03,  2.77it/s]    Loss : 3.36e+10:  82%|████████▏ | 41/50 [00:29<00:03,  2.77it/s]    Loss : 3.36e+10:  84%|████████▍ | 42/50 [00:29<00:02,  2.92it/s]    Loss : 3.36e+10:  84%|████████▍ | 42/50 [00:30<00:02,  2.92it/s]    Loss : 3.36e+10:  86%|████████▌ | 43/50 [00:30<00:02,  2.77it/s]    Loss : 3.35e+10:  86%|████████▌ | 43/50 [00:30<00:02,  2.77it/s]    Loss : 3.35e+10:  88%|████████▊ | 44/50 [00:30<00:02,  2.70it/s]    Loss : 3.34e+10:  88%|████████▊ | 44/50 [00:30<00:02,  2.70it/s]    Loss : 3.34e+10:  90%|█████████ | 45/50 [00:30<00:01,  2.86it/s]    Loss : 3.33e+10:  90%|█████████ | 45/50 [00:31<00:01,  2.86it/s]    Loss : 3.33e+10:  92%|█████████▏| 46/50 [00:31<00:01,  2.75it/s]    Loss : 3.32e+10:  92%|█████████▏| 46/50 [00:31<00:01,  2.75it/s]    Loss : 3.32e+10:  94%|█████████▍| 47/50 [00:31<00:01,  2.67it/s]    Loss : 3.32e+10:  94%|█████████▍| 47/50 [00:31<00:01,  2.67it/s]    Loss : 3.32e+10:  96%|█████████▌| 48/50 [00:31<00:00,  2.82it/s]    Loss : 3.32e+10:  96%|█████████▌| 48/50 [00:32<00:00,  2.82it/s]    Loss : 3.32e+10:  98%|█████████▊| 49/50 [00:32<00:00,  2.71it/s]    Loss : 3.32e+10:  98%|█████████▊| 49/50 [00:32<00:00,  2.71it/s]    Loss : 3.32e+10: 100%|██████████| 50/50 [00:32<00:00,  2.65it/s]    Loss : 3.32e+10: 100%|██████████| 50/50 [00:32<00:00,  1.53it/s]




.. GENERATED FROM PYTHON SOURCE LINES 71-73

We can see that we obtain the same PCA embedding (up to a rotation) using
both methods.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 34.442 seconds)


.. _sphx_glr_download_auto_examples_basics_demo_pca_via_affinity_matcher.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: demo_pca_via_affinity_matcher.ipynb <demo_pca_via_affinity_matcher.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: demo_pca_via_affinity_matcher.py <demo_pca_via_affinity_matcher.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
